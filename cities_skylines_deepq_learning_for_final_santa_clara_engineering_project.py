# -*- coding: utf-8 -*-
"""Cities:Skylines deepQ learning for Final Santa Clara Engineering Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WoK4VaXqXSZcRk8OcarftBprdg9IXgHF
"""

#pip install -q tf-agents

from __future__ import absolute_import, division, print_function

import base64
import imageio
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image
import threading
import sys
import win32pipe
import win32file
import win32event
import pywintypes
import time
import struct
import re

import tensorflow as tf

from tf_agents.agents.dqn import dqn_agent
from tf_agents.drivers import dynamic_step_driver
from tf_agents.environments import suite_gym
from tf_agents.environments import py_environment
from tf_agents.environments import tf_environment
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import sequential
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory
from tf_agents.specs import tensor_spec
from tf_agents.utils import common
from tf_agents.specs import array_spec
from tf_agents.environments import wrappers
from tf_agents.environments import suite_gym
from tf_agents.trajectories import time_step as ts
from tf_agents.environments import utils

from matplotlib import cm, colors
from time import sleep
from IPython.display import clear_output
from math import floor
from random import randrange

color_map = colors.ListedColormap(['green','gray','white','blue','black','orange','pink'])
plt.rcParams['figure.figsize'] = [20, 10]


#total_x = 28
#total_y = 30  

#map_x = 24
#map_y = 20

map_x = 5
map_y = 6
possible_actions = 5
#road_spaces = 136 
playable_squares = map_x * map_y
total_actions = playable_squares * possible_actions 

pmids = [
    "population", 
    "happiness",
    "life_span",
    "sheltered",
    "sick_count",
    "electricity_consumption",
    "water_consumption",
    "garbage",
    "unemployment",
    "criminal_amount",
    "extra_criminals",
    "education_1_capacity",
    "education_1_need",
    "education_1_rate",
    "education_2_capacity",
    "education_2_need",
    "education_2_rate",
    "education_3_capacity",
    "education_3_need",
    "education_3_rate",
    "water_pollution",
    "ground_pollution",
    "residential_demand",
    "commercial_demand",
    "workplace_demand",
    "actual_residential_demand",
    "actual_commercial_demand",
    "actual_workplace_demand"
]

pms = [0] * len(pmids)
pmd = {}

outqueue = [];


# Game board:
# 0 = Empty
# 1 = Road
# 2 = Residential Zone
# 3 = Commercial Zone
# 4 = Power Plant
# 5 = Hospital
# 6 = Wind Power Plant

class CitiesSkylinesEnvironment(py_environment.PyEnvironment):
  
  def __init__(self):
    """ 
    Action space is from 0 to 15679
    board space * possible actions
    62*62 = 3844, 3844 - 472, 3136 * 5 = 15680
    map_x * map_y = playable squares * possible_actions = total_actions - 1 
    """

    
    self._state = np.full((map_x,map_y),-1)
    self._game_cycles = 0;

    self._action_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=0, maximum=total_actions-1, name='action')
    self._observation_spec = array_spec.BoundedArraySpec(
        shape=(map_x,map_y), dtype=np.int32, minimum=0, maximum=possible_actions-1, name='observation')
    
    self._episode_ended = False
    self._reward_value = 0
    self._population = 0;
    self._pollution = 0;
    self._power = 0;
    self._income = 0;

    self._residential_count = 0
    self._commercial_count = 0
    self._coal_plant_count = 0
    self._wind_plant_count = 0
    self._hospital_count = 0

    self.firstreset = True;

    lock = threading.Lock()
    t1 = threading.Thread(target=self.iothread, args=(lock,))
    t1.start()
    i = input()

  def convertcoords(self, x, y):
    xret = 0;
    yret = 0;
    zret = 0;
    if x < 4:
      xret = 725 + 8 * y
      zret = 98 + 8 * x
    elif x > 19:
      xret = 722 + 8 * y
      zret = 131 + 8 * x
    elif y < 4:
      xret = 684 + 8 * y
      zret = 112 + 8 * x
    elif y < 12:
      xret = 699 + 8 * y
      zret = 113 + 8 * x
    else:
      xret = 716 + 8 * y
      zret = 115 + 8 * x
    coords = [xret,yret,zret]
    return coords
  def updatepms(self, st):
    l = st.split(", ")
    #print(st)
    #print(len(pms))
    #print(len(l))
    for i in range (len(pms)):
        pms[i] = l[i]
        pmd[pmids[i]] = pms[i]
    #print(pmd)

  def iothread(self, loc):
    pipealias1 = r'\\.\pipe\NP1'
    pipealias2 = r'\\.\pipe\NP2'


    bufsize = 1024*64

    notexiting = True
    while(notexiting):
        try:
            pipe = win32pipe.CreateNamedPipe(pipealias1,
                win32pipe.PIPE_ACCESS_DUPLEX,
                win32pipe.PIPE_TYPE_BYTE|win32pipe.PIPE_READMODE_BYTE|win32pipe.PIPE_WAIT, 
                1, bufsize, bufsize, 1, None) 
            try:
                print("waiting for connection on pipealias1")
                win32pipe.ConnectNamedPipe(pipe,None)
                print("connected to pipealias1")

                #print("connection established")
                while True:
                    inl = win32file.ReadFile(pipe, 4) #read the length of C# message
                    #print(inl)
                    leng = int.from_bytes(inl[1], sys.byteorder) 
                    #print(leng)
                    ins = win32file.ReadFile(pipe,leng) #read the C# message
                    #print('Read:',ins[1].decode())
                    self.updatepms(ins[1].decode())
                    st = "error"
                    loc.acquire()
                    if len(outqueue) == 0:
                        st = "noaction"
                    else:
                        print(outqueue[0])
                        st = outqueue[0]
                        outqueue.remove(outqueue[0])
                    loc.release()
                    send = struct.pack('I', len(st)) + st.encode()
                    win32file.WriteFile(pipe, send)
                    #print('wrote: ', send)
                    inw = win32file.ReadFile(pipe, 4) #the pipe will read its own messages
                    inw2 = win32file.ReadFile(pipe, len(st)) # the pipe will read its own messages
                    time.sleep(0.2)
            finally:
                win32file.CloseHandle(pipe)
        except Exception as ex:
            print("error with pipalias1: ",ex)
            try:
              pipe = win32pipe.CreateNamedPipe(pipealias2,
                  win32pipe.PIPE_ACCESS_DUPLEX,
                  win32pipe.PIPE_TYPE_BYTE|win32pipe.PIPE_READMODE_BYTE|win32pipe.PIPE_WAIT, 
                  1, bufsize, bufsize, 1, None) 
              try:
                print("waiting for connection on pipealias2")
                win32pipe.ConnectNamedPipe(pipe,None)
                print("connected to pipealias2")
                while True:
                    inl = win32file.ReadFile(pipe, 4) #read the length of C# message
                    #print(inl)
                    leng = int.from_bytes(inl[1], sys.byteorder) 
                    #print(leng)
                    ins = win32file.ReadFile(pipe,leng) #read the C# message
                    #print('Read:',ins[1].decode())
                    self.updatepms(ins[1].decode())
                    st = "error"
                    loc.acquire()
                    if len(outqueue) == 0:
                        st = "noaction"
                    else:
                        st = outqueue[0]
                        outqueue.remove(outqueue[0])
                    loc.release()
                    send = struct.pack('I', len(st)) + st.encode()
                    #print(str(send))
                    win32file.WriteFile(pipe, send)
                    #print('wrote: ', send)
                    inw = win32file.ReadFile(pipe, 4) #the pipe will read its own messages
                    inw2 = win32file.ReadFile(pipe, len(st)) # the pipe will read its own messages
                    time.sleep(1)

              finally:
                win32file.CloseHandle(pipe)
            except Exception as ex:
              print("error with pipalias2: ",ex)


  def action_spec(self):
    return self._action_spec

  def observation_spec(self):
    return self._observation_spec
  
  def current_time_step(self):
    return self._current_time_step

  def _reset(self):
      self._state = np.full((map_x,map_y),-1)
      self._episode_ended = False
      self._reward_value = 0
      self._population = 0
      self._pollution = 0
      self._power = 0
      self._income = 0
      self._residential_count = 0
      self._commercial_count = 0
      self._coal_plant_count = 0
      self._wind_plant_count = 0
      self._hospital_count = 0
      self._game_cycles = 0
      if self.firstreset == False:
        outqueue.append("reset")
      else:
        self.firstreset = False;
      return ts.restart(np.array(self._state, dtype=np.int32))

  def _step(self, action):
    #Add a while guard to pause the program 

    self._game_cycles += 1
    action_val = action // playable_squares

    if action_val == 0:
      # Residential zone
      self._population += 400
      self._pollution += 0
      self._residential_count += 100
      #Residences also provide income in form of taxes
      #About $7 per indivudal zone square, roughly 
      self._income += 700

    elif action_val == 1:
      # Commercial Zone
      self._income += 0.1*self._population
      self._pollution += 5
      self._commercial_count += 1  
      #print("Commercial Zone Built at " + str(x) + "," + str(y))
    elif action_val == 2:
      # Coal Power Plant
      #Placing one square at a time but in game takes up 30 squares, so should divide by 30... will do later once get loss function wokring 
      # 578
      #print(send)
      #time.sleep(5)
      # Coal Power Plant
      self._power += 40
      self._income -= 560
      self._pollution += 50
      self._coal_plant_count += 1

    elif action_val == 3:
      # Wind Power Plant
      # 573
      self._power += 30
      self._income -= 300
      self._wind_plant_count += 1

    elif action_val == 4:
      # Hospital
      # 1
      # self._population += floor(self._population * 0.15)
      # self._income -= 2000
      self._hospital_count += 1

    else:
      raise ValueError('`action` should be less than the max action value')

    flat_coord = action % playable_squares 
    x,y = self.map_action_to_coordinate(flat_coord)
    overwrite = False
    if self._state[x][y] != -1:
      overwrite = True
    self._state[x][y] = action_val  

    coords = self.convertcoords(x, y)
    if action_val == 0:
      send = "createzone|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) +"|2|1"
      outqueue.append(send)
    elif action_val == 1:
      send = "createzone|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) +"|4|1"
      outqueue.append(send)
    elif action_val == 2:
      send = "createbuilding|578|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
      outqueue.append(send)
    elif action_val == 3:
      send = "createbuilding|573|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
      outqueue.append(send)
    elif action_val == 4:
      send = "createbuilding|1|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
      outqueue.append(send)
    
    
    reward = self._calculate_reward(self.calculate_adjacency_bonus(x,y))
    if overwrite:
      reward -= 2000000
    #sleep(0.1)
    if self._game_cycles >= num_iterations:
      #print(reward)
      return ts.termination(np.array(self._state, dtype=np.int32), reward)
    else:
      return ts.transition(
          np.array(self._state, dtype=np.int32), reward, discount=1.0)

    
  def map_action_to_coordinate(self,action):
    x = action % map_x
    y = action // map_x 
    return(x,y)

    

  def _calculate_reward(self,adjacency_bonus):
    
    calculated_reward = 0
    power_demand = self._population * 10

    # Adjacency bonuses
    #calculated_reward += adjacency_bonus
    adjacency_bonus = 0
    
    calculated_reward += self._population * 100 - self._pollution
    if self._income < 0:
      calculated_reward -= calculated_reward * 0.75
    else: 
      calculated_reward += self._income / 10
    #Rewards decrease scales with difference between power supply and demand
    # if self._power < power_demand:
    #   calculated_reward -= (power_demand - self._power)
    # (calculated _reward * (power_demand - self._power) / power_demand)
    
    return calculated_reward

  def _calculate_reward2(self,adjacency_bonus):
    calculated_reward = 0

  def _printStats(self):
    print("Population: " + str(self._population))
    print("Power: " + str(self._power))
    print("Pollution: " + str(self._pollution))
    print("Income: " + str(floor(self._income)))

    print("Residentials: "+ str(self._residential_count))
    print("Commercials: "+ str(self._commercial_count))
    print("Coals: "+ str(self._coal_plant_count))
    print("Winds: "+ str(self._wind_plant_count))
    print("Hospitals: "+ str(self._hospital_count))
    print(floor(self._calculate_reward(0)))

  def calculate_adjacency_bonus(self,x,y):
    bonus_sum = 0
    directions = [(1,0),(0,1),(-1,0),(0,-1)]
    for direction in directions:
      new_x = x + direction[0]
      new_y = y + direction[1]
      if new_x > 0 and new_x < map_x and new_y > 0 and new_y <map_y:
        if self._state[new_x][new_y] == self._state[x][y]:
          bonus_sum += 100
    return bonus_sum

  def show_map(self):
    map_view_state = [[0 for x in range(map_x)] for y in range(map_y)]
    for x in range(map_x):
      for y in range(map_y):
        #if x <= 1 or y <= 1 or x >= 60 or y >= 60 or x >= 30 and x <=31 or y >= 30 and y <= 31:
        #  map_view_state[x][y] = 1
        #else:
        #  transformed_x = x
        #  transformed_y = y
        #  if x <= 29 and y <= 29:
        #    transformed_x -= 2
        #   transformed_y -= 2
        #  elif x <= 29:
        #    transformed_x -= 2
        #    transformed_y -= 4
        #  elif y <= 29:
        #    transformed_x -= 4
        #    transformed_y -= 2
        #  else:
        #    transformed_x -= 4
        #    transformed_y -= 4
        map_view_state[x][y] = self._state[x][y]
    self._printStats()
    plt.imshow(map_view_state, cmap = color_map)
    plt.show()
    sleep(.5)
    clear_output(wait=True)
  
def main():
  plt.figure()
  
  env = CitiesSkylinesEnvironment()
  for x in range(500):
    action = randrange(0,total_actions)
    env._step(action)

    if x % 50 == 0:
      env.show_map()
      


# main()

env = CitiesSkylinesEnvironment()
tf_env = tf_py_environment.TFPyEnvironment(env)
train_env = tf_env
eval_env = tf_env

num_iterations =  30# @param {type:"integer"}

#initial_collect_steps = int(num_iterations/10) # @param {type:"integer"} 
initial_collect_steps = 25
collect_steps_per_iteration = 10  # @param {type:"integer"}
replay_buffer_max_length = 100000  # @param {type:"integer"}

batch_size = 64  # @param {type:"integer"}
learning_rate = 1e-3  # @param {type:"number"}
log_interval = 5  # @param {type:"integer"}

num_eval_episodes = 30  # @param {type:"integer"}
eval_interval =  int(num_iterations/10) # @param {type:"integer"}

# Consider layers that go big -> small -> big, e.g.
fc_layer_params = (1024, 256, 64, 256, 1024)
#fc_layer_params = (100, 50)
# Maybe change env -> tf_env
action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())
num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1

# Define a helper function to create Dense layers configured with the right
# activation and kernel initializer.
def dense_layer(num_units):
  return tf.keras.layers.Dense(
      num_units,
      activation=tf.keras.activations.relu,
      kernel_initializer=tf.keras.initializers.VarianceScaling(
          scale=2.0, mode='fan_in', distribution='truncated_normal'))

# QNetwork consists of a sequence of Dense layers followed by a dense layer
# with `num_actions` units to generate one q_value per available action as
# it's output.
flatten_layer = tf.keras.layers.Flatten()
dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]
q_values_layer = tf.keras.layers.Dense(
    num_actions,
    activation=None,
    kernel_initializer=tf.keras.initializers.RandomUniform(
        minval=-0.03, maxval=0.03),
    bias_initializer=tf.keras.initializers.Constant(-0.2))
q_net = sequential.Sequential([flatten_layer] + dense_layers + [q_values_layer])

optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)

train_step_counter = tf.Variable(0)
print(q_net)

print(tf_env.time_step_spec())
print(tf_env.action_spec())
print(q_net)
print(q_net.losses)

agent = dqn_agent.DqnAgent(
    tf_env.time_step_spec(),
    tf_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=train_step_counter)

agent.initialize()

eval_policy = agent.policy
collect_policy = agent.collect_policy
random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),
                                                train_env.action_spec())
#example_environment = tf_py_environment.TFPyEnvironment(CitiesSkylinesEnvironment)
#time_step = example_environment.reset()
#time_step = tf_env.reset()
#random_policy.action(time_step)

def compute_avg_return(environment, policy, num_episodes=10):

  total_return = 0.0
  for _ in range(num_episodes):

    time_step = environment.reset()
    episode_return = 0.0

    while not time_step.is_last():
      action_step = policy.action(time_step)
      time_step = environment.step(action_step.action)
      episode_return += time_step.reward
    total_return += episode_return

  avg_return = total_return / num_episodes
  return avg_return.numpy()[0]

compute_avg_return(eval_env, random_policy, num_eval_episodes)

replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=train_env.batch_size,
    max_length=replay_buffer_max_length)

agent.collect_data_spec

agent.collect_data_spec._fields

def collect_step(environment, policy, buffer):
  time_step = environment.current_time_step()
  action_step = policy.action(time_step)
  #action_num = action_step.action.numpy()[0]
  # print(env.map_action_to_coordinate(action_num // playable_squares))
  # print(floor(action_num/playable_squares))
  next_time_step = environment.step(action_step.action)
  traj = trajectory.from_transition(time_step, action_step, next_time_step)

  # Add trajectory to the replay buffer
  buffer.add_batch(traj)

def collect_data(env, policy, buffer, steps):
  for _ in range(steps):
    collect_step(env, policy, buffer)

collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)

dataset = replay_buffer.as_dataset(
    num_parallel_calls=3, 
    sample_batch_size=batch_size, 
    num_steps=2).prefetch(3)


dataset
iterator = iter(dataset)
print(iterator)

# Commented out IPython magic to ensure Python compatibility.


# (Optional) Optimize by wrapping some of the code in a graph using TF function.
agent.train = common.function(agent.train)

# Reset the train step
agent.train_step_counter.assign(0)

# Evaluate the agent's policy once before training.
avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
returns = [avg_return]

for x in range(num_iterations):

  # Collect a few steps using collect_policy and save to the replay buffer.
  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)

  #td_targets = dqn_agent.compute_td_targets(q_values_layer, agent.time_step_spec.reward, agent.time_step_spec.discount)

  # Sample a batch of data from the buffer and update the agent's network.
  experience, unused_info = next(iterator)
  train_loss = agent.train(experience).loss

  step = agent.train_step_counter.numpy()

  if step % log_interval == 0:
    print('step = {0}: loss = {1}'.format(step, train_loss))


  if step % eval_interval == 0:
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    print('step = {0}: Average Return = {1}'.format(step, avg_return))
    returns.append(avg_return)
    # print(eval_env.render())
    #eval_env._env.envs[0].show_map()

iterations = range(0, num_iterations + 1, eval_interval)
plt.plot(iterations, returns)
plt.ylabel('Average Return')
plt.xlabel('Iterations')
# plt.ylim(top=250)

def create_policy_eval_(policy, num_episodes=5):
  for x in range(num_episodes):
    count = 0
    time_step = eval_env.reset()
    while not time_step.is_last():
      count+=1
      action_step = policy.action(time_step)
      time_step = eval_env.step(action_step.action)
      print(str(time_step))
      #eval_env._env.envs[-1].show_map()

create_policy_eval_(agent.policy)