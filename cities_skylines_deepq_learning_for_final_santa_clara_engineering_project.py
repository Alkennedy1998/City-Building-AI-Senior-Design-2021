# -*- coding: utf-8 -*-
"""Cities:Skylines deepQ learning for Final Santa Clara Engineering Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WoK4VaXqXSZcRk8OcarftBprdg9IXgHF
"""

#pip install -q tf-agents

from __future__ import absolute_import, division, print_function

import base64
import imageio
import IPython
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import PIL.Image
import threading
import sys
import win32pipe
import win32file
import win32event
import pywintypes
import time
import struct
import re

import tensorflow as tf

from tf_agents.agents.dqn import dqn_agent
from tf_agents.drivers import dynamic_step_driver
from tf_agents.environments import suite_gym
from tf_agents.environments import py_environment
from tf_agents.environments import tf_environment
from tf_agents.environments import tf_py_environment
from tf_agents.eval import metric_utils
from tf_agents.metrics import tf_metrics
from tf_agents.networks import sequential
from tf_agents.policies import random_tf_policy
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.trajectories import trajectory
from tf_agents.specs import tensor_spec
from tf_agents.utils import common
from tf_agents.specs import array_spec
from tf_agents.environments import wrappers
from tf_agents.environments import suite_gym
from tf_agents.trajectories import time_step as ts
from tf_agents.environments import utils

from matplotlib import cm, colors
from time import sleep
from datetime import datetime
from IPython.display import clear_output
from math import floor
from random import randrange

import os 

color_map = colors.ListedColormap(['green','gray','white','blue','black','orange','pink'])
plt.rcParams['figure.figsize'] = [20, 10]

map_x = 24
map_y = 20

possible_actions = 5

playable_squares = map_x * map_y
total_actions = playable_squares * possible_actions 

log_all_actions = True
def setLAA(a):
  global log_all_actions
  log_all_actions = a

log_all_steps = True
def setLAS(a):
  global log_all_steps
  log_all_steps = a

num_iterations = 100
def setNI(a):
  global num_iterations
  num_iterations = a

def num_iterations():
  global num_iterations
  return num_iterations

initial_collect_steps = 100

def setICS(a):
  global initial_collect_steps
  initial_collect_steps = a

def initial_collect_steps():
  global initial_collect_steps
  return initial_collect_steps

time_between_actions = 7

def setTBA(a):
  global time_between_actions
  time_between_actions = a

time_after_last = 30

def setTAL(a):
  global time_after_last
  time_after_last = a

reset_buffer_time = 10

def setRBT(a):
  global reset_buffer_time
  reset_buffer_time = a

logfile = "undefined.txt"

connected = False

#order of performance measures
pmids = [
    "population", 
    "happiness",
    "life_span",
    "sheltered",
    "sick_count",
    "electricity_consumption",
    "water_consumption",
    "garbage",
    "unemployment",
    "criminal_amount",
    "extra_criminals",
    "education_1_capacity",
    "education_1_need",
    "education_1_rate",
    "education_2_capacity",
    "education_2_need",
    "education_2_rate",
    "education_3_capacity",
    "education_3_need",
    "education_3_rate",
    "water_pollution",
    "ground_pollution",
    "residential_demand",
    "commercial_demand",
    "workplace_demand",
    "actual_residential_demand",
    "actual_commercial_demand",
    "actual_workplace_demand"
]

#weights for each performance measure 
w = [
  1.0,
  3.0,
  2.0,
  1.0,
  -2.0,
  1.0,
  1.0,
  -1.0,
  -2.0,
  -3.0,
  -3.0,
  0.0,
  0.0,
  0.0,
  0.0,
  0.0,
  0.0,
  0.0,
  0.0,
  0.0,
  -1.0,
  -1.0,
  0.0,
  0.0,
  0.0,
  -4.0,
  -4.0,
  -4.0
]

pms = [0] * len(pmids)
pmd = {}

#action queue to be sent to C# script
outqueue = [];

resetting_allowed = True;


# Game board:
# 0 = Empty
# 1 = Road
# 2 = Residential Zone
# 3 = Commercial Zone
# 4 = Power Plant
# 5 = Hospital
# 6 = Wind Power Plant
def connection():
  return connected;
def setConnection(c):
  global connected
  connected = c

class CitiesSkylinesEnvironment(py_environment.PyEnvironment):

  def __init__(self):
    """ 
    Action space is from 0 to 15679
    board space * possible actions
    62*62 = 3844, 3844 - 472, 3136 * 5 = 15680
    map_x * map_y = playable squares * possible_actions = total_actions - 1 
    """

    
    self._state = np.full((map_x,map_y),-1)
    self._game_cycles = 0;

    self._action_spec = array_spec.BoundedArraySpec(
        shape=(), dtype=np.int32, minimum=0, maximum=total_actions-1, name='action')
    self._observation_spec = array_spec.BoundedArraySpec(
        shape=(map_x,map_y), dtype=np.int32, minimum=0, maximum=possible_actions-1, name='observation')
    
    self._episode_ended = False
    self._reward_value = 0
    self._population = 0;
    self._pollution = 0;
    self._power = 0;
    self._income = 0;

    self._residential_count = 0
    self._commercial_count = 0
    self._coal_plant_count = 0
    self._wind_plant_count = 0
    self._hospital_count = 0

    self.firstreset = True;

    lock = threading.Lock()
    if(connection()==False):
      t1 = threading.Thread(target=self.iothread, args=(lock,))
      t1.start()
      ready = False
      while(ready == False):
        lock.acquire()
        if(connection() == True):
          ready = True
        lock.release()
        time.sleep(1)

    

  def convertcoords(self, x, y):
    xret = 0;
    yret = 0;
    zret = 0;
    if x < 4:
      xret = 725 + 8 * y
      zret = 98 + 8 * x
    elif x > 19:
      xret = 722 + 8 * y
      zret = 131 + 8 * x
    elif y < 4:
      xret = 684 + 8 * y
      zret = 112 + 8 * x
    elif y < 12:
      xret = 699 + 8 * y
      zret = 113 + 8 * x
    else:
      xret = 716 + 8 * y
      zret = 115 + 8 * x
    coords = [xret,yret,zret]
    return coords
  def updatepms(self, st):
    l = st.split(", ")
    for i in range (len(pms)):
        pms[i] = l[i]
        pmd[pmids[i]] = pms[i]

  def iothread(self, loc):
    pipealias1 = r'\\.\pipe\NP1'
    pipealias2 = r'\\.\pipe\NP2'


    bufsize = 1024*64

    notexiting = True
    while(notexiting):
        loc.acquire()
        setConnection(False)
        loc.release()
        try:
            pipe = win32pipe.CreateNamedPipe(pipealias1,
                win32pipe.PIPE_ACCESS_DUPLEX,
                win32pipe.PIPE_TYPE_BYTE|win32pipe.PIPE_READMODE_BYTE|win32pipe.PIPE_WAIT, 
                1, bufsize, bufsize, 1, None) 
            try:
                print("waiting for connection on pipealias1 (please start Cities: Skylines)")
                win32pipe.ConnectNamedPipe(pipe,None)
                print("connected to pipealias1")
                loc.acquire()
                setConnection(True)
                loc.release()
                print("connection established, connected is " + str(connection()))
                while True:
                    inl = win32file.ReadFile(pipe, 4) #read the length of C# message
                    leng = int.from_bytes(inl[1], sys.byteorder) 
                    ins = win32file.ReadFile(pipe,leng) #read the C# message
                    
                    self.updatepms(ins[1].decode())
                    st = "error"
                    loc.acquire()
                    if len(outqueue) == 0:
                        st = "noaction"
                    else:
                        st = outqueue[0]
                        outqueue.remove(outqueue[0])
                    loc.release()
                    r = False
                    r = (st == "reset")
                    send = struct.pack('I', len(st)) + st.encode()
                    win32file.WriteFile(pipe, send)
                    inw = win32file.ReadFile(pipe, 4) #the pipe will read its own messages
                    inw2 = win32file.ReadFile(pipe, len(st)) # the pipe will read its own messages
                    #if r:
                      #break
                    time.sleep(0.2)
            finally:
                win32file.CloseHandle(pipe)
        except Exception as ex:
            print("error with pipalias1: ",ex)
            loc.acquire()
            setConnection(False)
            loc.release()
            try:
              pipe = win32pipe.CreateNamedPipe(pipealias2,
                  win32pipe.PIPE_ACCESS_DUPLEX,
                  win32pipe.PIPE_TYPE_BYTE|win32pipe.PIPE_READMODE_BYTE|win32pipe.PIPE_WAIT, 
                  1, bufsize, bufsize, 1, None) 
              try:
                print("waiting for connection on pipealias2 (please start Cities: Skylines)")
                win32pipe.ConnectNamedPipe(pipe,None)
                print("connected to pipealias2")
                loc.acquire()
                setConnection(True)
                loc.release()
                while True:
                    inl = win32file.ReadFile(pipe, 4) #read the length of C# message
                    leng = int.from_bytes(inl[1], sys.byteorder) 
                    ins = win32file.ReadFile(pipe,leng) #read the C# message
                    self.updatepms(ins[1].decode())
                    st = "error"
                    loc.acquire()
                    if len(outqueue) == 0:
                        st = "noaction"
                    else:
                        st = outqueue[0]
                        outqueue.remove(outqueue[0])
                    loc.release()
                    send = struct.pack('I', len(st)) + st.encode()
                    win32file.WriteFile(pipe, send)
                    inw = win32file.ReadFile(pipe, 4) #the pipe will read its own messages
                    inw2 = win32file.ReadFile(pipe, len(st)) # the pipe will read its own messages
                    time.sleep(1)

              finally:
                win32file.CloseHandle(pipe)
            except Exception as ex:
              print("error with pipalias2: ",ex)


  def action_spec(self):
    return self._action_spec

  def observation_spec(self):
    return self._observation_spec
  
  def current_time_step(self):
    return self._current_time_step

  def _reset(self):
      if self.firstreset == False:
        time.sleep(time_after_last)
      self._state = np.full((map_x,map_y),-1)
      self._episode_ended = False
      self._reward_value = 0
      self._population = 0
      self._pollution = 0
      self._power = 0
      self._income = 0
      self._residential_count = 0
      self._commercial_count = 0
      self._coal_plant_count = 0
      self._wind_plant_count = 0
      self._hospital_count = 0
      self._game_cycles = 0
      self.turbines = 0;
      self.coal = 0;
      self.turbines_max = 3;
      self.coal_max = 3;
      finalscore = self._calculate_reward2()
      
      pms = [0] * len(pmids)
      if self.firstreset == False and resetting_allowed == True:
        print("final score before reset: " + str(finalscore))
        outqueue.append("reset")
        if(log_all_steps == True or log_all_actions == True):
          print("appending reset..")
        time.sleep(reset_buffer_time)
      else:
        self.firstreset = False;
      return ts.restart(np.array(self._state, dtype=np.int32))

  def _step(self, action):
    #Add a while guard to pause the program 

    self._game_cycles += 1
    action_val = action // playable_squares

    flat_coord = action % playable_squares 
    x,y = self.map_action_to_coordinate(flat_coord)
    overwrite = False
    if self._state[x][y] != -1:
      overwrite = True
    self._state[x][y] = action_val  

    coords = self.convertcoords(x, y)
    if action_val == 0:
      send = "createzone|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) +"|2|16"
      outqueue.append(send)
    elif action_val == 1:
      send = "createzone|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) +"|4|16"
      outqueue.append(send)
    elif action_val == 2: #coal
      if(self.coal <= self.coal_max):
        send = "createbuilding|578|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
        outqueue.append(send)
        self.coal += 1

    elif action_val == 3: #wind
      if(self.turbines <= self.turbines_max):
        send = "createbuilding|573|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
        outqueue.append(send)
        self.turbines += 1
    elif action_val == 4: #hospital
      send = "createbuilding|1|" + str(coords[0]) + "," + str(coords[1]) + "," + str(coords[2]) + "|0|0"
      outqueue.append(send)
    
    time.sleep(time_between_actions)
    reward = self._calculate_reward2()
    print(str(self._game_cycles) + ", " + str(reward) + ", " + str(action_val) + str(coords))
    if log_all_actions == True:
      global logfile
      with open(logfile, 'a') as wf:
        actionString = "\t\t"+str(self._game_cycles) + ", " + str(reward) + ", " + str(action_val) + str(coords) + "\n"
        wf.write(actionString)
        wf.close()
    if overwrite:
      reward -= 000000
    #sleep(0.1)
    if self._game_cycles >= num_iterations:
      #print(reward)
      return ts.termination(np.array(self._state, dtype=np.int32), reward)
    else:
      return ts.transition(
          np.array(self._state, dtype=np.int32), reward, discount=1.0)

    
  def map_action_to_coordinate(self,action):
    x = action % map_x
    y = action // map_x 
    return(x,y)

    

  def _calculate_reward(self,adjacency_bonus):
    
    calculated_reward = 0.0
    power_demand = self._population * 10

    # Adjacency bonuses
    #calculated_reward += adjacency_bonus
    adjacency_bonus = 0
    
    calculated_reward += self._population * 100 - self._pollution
    if self._income < 0:
      calculated_reward -= calculated_reward * 0.75
    else: 
      calculated_reward += self._income / 10
    #Rewards decrease scales with difference between power supply and demand
    # if self._power < power_demand:
    #   calculated_reward -= (power_demand - self._power)
    # (calculated _reward * (power_demand - self._power) / power_demand)
    
    return calculated_reward

  def _calculate_reward2(self):
    calculated_reward = 0.0
    for el in range(len(pms)):
      calculated_reward += float(pms[el]) * float(w[el])
      #print(str(el) + ": " + str(calculated_reward))
    return calculated_reward

  def _printStats(self):
    print("Population: " + str(self._population))
    print("Power: " + str(self._power))
    print("Pollution: " + str(self._pollution))
    print("Income: " + str(floor(self._income)))

    print("Residentials: "+ str(self._residential_count))
    print("Commercials: "+ str(self._commercial_count))
    print("Coals: "+ str(self._coal_plant_count))
    print("Winds: "+ str(self._wind_plant_count))
    print("Hospitals: "+ str(self._hospital_count))
    print(floor(self._calculate_reward(0)))

  def calculate_adjacency_bonus(self,x,y):
    bonus_sum = 0
    directions = [(1,0),(0,1),(-1,0),(0,-1)]
    for direction in directions:
      new_x = x + direction[0]
      new_y = y + direction[1]
      if new_x > 0 and new_x < map_x and new_y > 0 and new_y <map_y:
        if self._state[new_x][new_y] == self._state[x][y]:
          bonus_sum += 100
    return bonus_sum

  def show_map(self):
    map_view_state = [[0 for x in range(map_x)] for y in range(map_y)]
    for x in range(map_x):
      for y in range(map_y):
        #if x <= 1 or y <= 1 or x >= 60 or y >= 60 or x >= 30 and x <=31 or y >= 30 and y <= 31:
        #  map_view_state[x][y] = 1
        #else:
        #  transformed_x = x
        #  transformed_y = y
        #  if x <= 29 and y <= 29:
        #    transformed_x -= 2
        #   transformed_y -= 2
        #  elif x <= 29:
        #    transformed_x -= 2
        #    transformed_y -= 4
        #  elif y <= 29:
        #    transformed_x -= 4
        #    transformed_y -= 2
        #  else:
        #    transformed_x -= 4
        #    transformed_y -= 4
        map_view_state[x][y] = self._state[x][y]
    self._printStats()
    plt.imshow(map_view_state, cmap = color_map)
    plt.show()
    sleep(.5)
    clear_output(wait=True)

def configure():
  scriptlocation = os.path.dirname(os.path.realpath(__file__))
  logfilename = datetime.now().strftime('%Y-%m-%d_%H%M%S.txt')
  logfilename = os.path.join('logs',logfilename)
  global logfile
  logfile = os.path.join(scriptlocation, logfilename)
  print("log file created at " + logfile)
  with open(logfile, 'w') as wf:
    title = "Runtime logs for iteration started at " + datetime.now().strftime('%H:%M:%S, %Y-%m-%d.\n')
    wf.write(title)
    wf.close()
  try:
    filename = "skylinesconfig.txt"
    filepath = os.path.join(scriptlocation, filename)
    print("config file found at " + filepath)
    with open(filepath) as f:
      lis = [line.split() for line in f]
      for l in lis:
        #print(l[0])
        #print(l[1])
        if(len(l) < 2): 
          print()
        elif (l[0] == "num_iterations="): 
          try:
            a = int(l[1])
            setNI(a)
          except:
            print("config error, " + l[1] + "is not an integer")
        elif (l[0] == "initial_collect_steps="): 
          try:
            a = int(l[1])
            setICS(a)
          except:
            print("config error, " + l[1] + "is not an integer")
        elif (l[0] == "time_between_actions="): 
          try:
            a = int(l[1])
            setTBA(a)
          except:
            print("config error, " + l[1] + "is not an integer")
        elif (l[0] == "time_after_last="): 
          try:
            a = int(l[1])
            setTAL(a)
          except:
            print("config error, " + l[1] + "is not an integer")
        elif (l[0] == "reset_buffer_time="): 
          try:
            a = int(l[1])
            setRBT(a)
          except:
            print("config error, " + l[1] + "is not an integer")
        elif (l[0] == "log_all_steps="):
          try:
            a = bool(l[1])
            setLAS(a)
          except:
            print("config error, " + l[1] + "is not a bool")
        elif (l[0] == "log_all_actions="):
          try:
            a = bool(l[1])
            setLAA(a)
          except:
            print("config error, " + l[1] + "is not a bool")
                        
        #else:
          #print("it was nothing")
  except:
    print("could not locate skylinesconfig.txt")

def main():
  plt.figure()
  
  env = CitiesSkylinesEnvironment()
  for x in range(500):
    action = randrange(0,total_actions)
    env._step(action)

    if x % 50 == 0:
      env.show_map()
      


# main()

configure()

env = CitiesSkylinesEnvironment()
tf_env = tf_py_environment.TFPyEnvironment(env)
train_env = tf_env
eval_env = tf_env

#num_iterations =  100   set above    # @param {type:"integer"}

#initial_collect_steps = int(num_iterations/10) # @param {type:"integer"} 
#initial_collect_steps = 100 set above
collect_steps_per_iteration = 1  # @param {type:"integer"}
replay_buffer_max_length = 100000  # @param {type:"integer"}

batch_size = 64  # @param {type:"integer"}
learning_rate = 1e-3  # @param {type:"number"}
log_interval = 1  # @param {type:"integer"}

num_eval_episodes = 10  # @param {type:"integer"}
eval_interval =  int(num_iterations/10) # @param {type:"integer"}

# Consider layers that go big -> small -> big, e.g.
fc_layer_params = (1024, 256, 64, 256, 1024)
#fc_layer_params = (100, 50)
# Maybe change env -> tf_env
action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())
num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1
#configure()
# Define a helper function to create Dense layers configured with the right
# activation and kernel initializer.



def dense_layer(num_units):
  return tf.keras.layers.Dense(
      num_units,
      activation=tf.keras.activations.relu,
      kernel_initializer=tf.keras.initializers.VarianceScaling(
          scale=2.0, mode='fan_in', distribution='truncated_normal'))

# QNetwork consists of a sequence of Dense layers followed by a dense layer
# with `num_actions` units to generate one q_value per available action as
# it's output.
flatten_layer = tf.keras.layers.Flatten()
dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]
q_values_layer = tf.keras.layers.Dense(
    num_actions,
    activation=None,
    kernel_initializer=tf.keras.initializers.RandomUniform(
        minval=-0.03, maxval=0.03),
    bias_initializer=tf.keras.initializers.Constant(-0.2))
q_net = sequential.Sequential([flatten_layer] + dense_layers + [q_values_layer])

optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)

train_step_counter = tf.Variable(0)
#print(q_net)

#print(tf_env.time_step_spec())
#print(tf_env.action_spec())
#print(q_net)
#print(q_net.losses)

agent = dqn_agent.DqnAgent(
    tf_env.time_step_spec(),
    tf_env.action_spec(),
    q_network=q_net,
    optimizer=optimizer,
    td_errors_loss_fn=common.element_wise_squared_loss,
    train_step_counter=train_step_counter)

agent.initialize()
eval_policy = agent.policy
collect_policy = agent.collect_policy
random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),
                                                train_env.action_spec())

example_environment = tf_py_environment.TFPyEnvironment(CitiesSkylinesEnvironment)
time_step = example_environment.reset()
time_step = tf_env.reset()
random_policy.action(time_step)

def compute_avg_return(environment, policy, num_episodes=10):

  total_return = 0.0
  global logfile
  if log_all_steps == True:
      with open(logfile, 'a') as wf:
        actionString = "Avg return step\n"
        wf.write(actionString)
        wf.close()
  for s in range(num_episodes):
    if log_all_steps == True:
      with open(logfile, 'a') as wf:
        actionString = "\tStep " + str(s) + "\n"
        wf.write(actionString)
        wf.close()
    time_step = environment.reset()
    episode_return = 0.0

    while not time_step.is_last():
      action_step = policy.action(time_step)
      time_step = environment.step(action_step.action)
      episode_return += time_step.reward
    total_return += episode_return

  avg_return = total_return / num_episodes
  return avg_return.numpy()[0]

compute_avg_return(eval_env, random_policy, num_eval_episodes)

replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=train_env.batch_size,
    max_length=replay_buffer_max_length)

agent.collect_data_spec

agent.collect_data_spec._fields

def collect_step(environment, policy, buffer):
  time_step = environment.current_time_step()
  action_step = policy.action(time_step)
  #action_num = action_step.action.numpy()[0]
  # print(env.map_action_to_coordinate(action_num // playable_squares))
  # print(floor(action_num/playable_squares))
  next_time_step = environment.step(action_step.action)
  traj = trajectory.from_transition(time_step, action_step, next_time_step)

  # Add trajectory to the replay buffer
  buffer.add_batch(traj)

def collect_data(env, policy, buffer, steps):
  for s in range(steps):
    if log_all_steps == True:
      global logfile
      with open(logfile, 'a') as wf:
        actionString = "\tStep " + str(s) + "\n"
        wf.write(actionString)
        wf.close()
    collect_step(env, policy, buffer)

collect_data(train_env, random_policy, replay_buffer, initial_collect_steps())

dataset = replay_buffer.as_dataset(
    num_parallel_calls=3, 
    sample_batch_size=batch_size, 
    num_steps=2).prefetch(3)


dataset
iterator = iter(dataset)
#print(iterator)

# Commented out IPython magic to ensure Python compatibility.


# (Optional) Optimize by wrapping some of the code in a graph using TF function.
agent.train = common.function(agent.train)

# Reset the train step
agent.train_step_counter.assign(0)

# Evaluate the agent's policy once before training.
avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
returns = [avg_return]
#returns = []

for x in range(num_iterations):

  # Collect a few steps using collect_policy and save to the replay buffer.
  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)

  #td_targets = dqn_agent.compute_td_targets(q_values_layer, agent.time_step_spec.reward, agent.time_step_spec.discount)

  # Sample a batch of data from the buffer and update the agent's network.
  experience, unused_info = next(iterator)
  train_loss = agent.train(experience).loss

  step = agent.train_step_counter.numpy()

  if step % log_interval == 0:
    print('step = {0}: loss = {1}'.format(step, train_loss))
    
    with open(logfile, 'a') as wf:
      actionString = '\tstep = {0}: loss = {1}'.format(step, train_loss)
      wf.write(actionString)
      wf.close()



  if step % eval_interval == 0:
    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)
    print('step = {0}: Average Return = {1}'.format(step, avg_return))
    returns.append(avg_return)
    # print(eval_env.render())
    #eval_env._env.envs[0].show_map()

iterations = range(0, num_iterations + 1, eval_interval)
plt.plot(iterations, returns)
plt.ylabel('Average Return')
plt.xlabel('Iterations')
# plt.ylim(top=250)

def create_policy_eval_(policy, num_episodes=5):
  for x in range(num_episodes):
    count = 0
    time_step = eval_env.reset()
    while not time_step.is_last():
      count+=1
      action_step = policy.action(time_step)
      time_step = eval_env.step(action_step.action)
      #print(str(time_step))
      eval_env._env.envs[-1].show_map()

create_policy_eval_(agent.policy)